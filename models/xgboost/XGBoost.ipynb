{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_directory = \"./figures\"\n",
    "os.makedirs(figure_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Annual Electricity Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_annual_demand_folder = \"./data/annual_electricity_demand/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_annual_files = [\n",
    "    file_name\n",
    "    for file_name in os.listdir(electricity_annual_demand_folder)\n",
    "    if file_name.endswith(\".parquet\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_annual_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_demand = pandas.DataFrame()\n",
    "\n",
    "for file_name in tqdm(ed_annual_files):\n",
    "    df_current = pandas.read_parquet(\n",
    "        electricity_annual_demand_folder + file_name\n",
    "    )\n",
    "\n",
    "    df_current = df_current.resample(\n",
    "        \"1h\", label=\"right\", closed=\"right\"\n",
    "    ).mean()\n",
    "\n",
    "    # Add a column for the region name\n",
    "    df_current[\"region_code\"] = file_name.split(\".\")[0]\n",
    "\n",
    "    # Reset index to move \"Time (UTC)\" to a column\n",
    "    df_current = df_current.reset_index()\n",
    "\n",
    "    df_annual_demand = pandas.concat(\n",
    "        [df_annual_demand, df_current], ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_annual_demand.shape)\n",
    "df_annual_demand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_folder = \"./data/temperature/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_files = [\n",
    "    file_name\n",
    "    for file_name in os.listdir(temperature_folder)\n",
    "    if file_name.endswith(\".parquet\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_temperature = pandas.DataFrame()\n",
    "\n",
    "for file_name in tqdm(temperature_files):\n",
    "    df_current = pandas.read_parquet(temperature_folder + file_name)\n",
    "\n",
    "    # Add a column for the region name\n",
    "    df_current[\"region_code\"] = file_name.split(\"_temp\")[0]\n",
    "\n",
    "    # Reset index to move \"Time (UTC)\" to a column\n",
    "    df_current = df_current.reset_index()\n",
    "\n",
    "    df_all_temperature = pandas.concat(\n",
    "        [df_all_temperature, df_current], ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_temperature.shape)\n",
    "df_all_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### GDP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Read in gdp data\n",
    "with open(\"./data/gdp_data.pkl\", \"rb\") as f:\n",
    "    gdp_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp_data = pandas.DataFrame()\n",
    "\n",
    "for country_code, region_dictionary in gdp_data.items():\n",
    "    for region_code, gdp_series in region_dictionary.items():\n",
    "        df_current = pandas.DataFrame(gdp_series).reset_index()\n",
    "        df_current.columns = [\"year\", \"GDP\"]\n",
    "        df_current[\"year\"] = df_current[\"year\"].astype(int)\n",
    "\n",
    "        df_current[\"region_code\"] = region_code\n",
    "        df_current[\"country_code\"] = country_code\n",
    "\n",
    "        df_gdp_data = pandas.concat(\n",
    "            [df_gdp_data, df_current], ignore_index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_gdp_data.shape)\n",
    "df_gdp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Electricity Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_demand_folder = \"./data/electricity_demand/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_files = [\n",
    "    file_name\n",
    "    for file_name in os.listdir(electricity_demand_folder)\n",
    "    if file_name.endswith(\".parquet\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand = pandas.DataFrame()\n",
    "\n",
    "for file_name in tqdm(demand_files):\n",
    "    df_current = pandas.read_parquet(electricity_demand_folder + file_name)\n",
    "\n",
    "    df_current[\"Load (MW)\"] = df_current[\"Load (MW)\"].astype(float)\n",
    "\n",
    "    df_current = df_current.resample(\n",
    "        \"1h\", label=\"right\", closed=\"right\"\n",
    "    ).mean()\n",
    "\n",
    "    # Add a column for the region name\n",
    "    df_current[\"region_code\"] = str.join(\"_\", file_name.split(\"_\")[:-1])\n",
    "\n",
    "    # Reset index to move \"Time (UTC)\" to a column\n",
    "    df_current = df_current.reset_index()\n",
    "\n",
    "    df_demand = pandas.concat([df_demand, df_current], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_demand.shape)\n",
    "df_demand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_demand = df_annual_demand.sort_values(by=[\"Time (UTC)\"])\n",
    "df_all_temperature = df_all_temperature.sort_values(by=[\"Time (UTC)\"])\n",
    "df_demand = df_demand.sort_values(by=[\"Time (UTC)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pandas.merge(\n",
    "    df_all_temperature, df_annual_demand, on=[\"Time (UTC)\", \"region_code\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset.shape)\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pandas.merge(\n",
    "    combined_dataset, df_demand, on=[\"Time (UTC)\", \"region_code\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset.shape)\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pandas.merge(\n",
    "    combined_dataset,\n",
    "    df_gdp_data.drop(columns=[\"country_code\"]),\n",
    "    left_on=[\"Local year\", \"region_code\"],\n",
    "    right_on=[\"year\", \"region_code\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset.shape)\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.drop(columns=[\"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = len(combined_dataset)\n",
    "print(\"Before removing duplicates:\", row_count)\n",
    "combined_dataset = combined_dataset.drop_duplicates(\n",
    "    subset=[col for col in combined_dataset.columns if col != \"Load (MW)\"]\n",
    ")\n",
    "print(\"Without duplicates: \", len(combined_dataset))\n",
    "print(\"Difference\", row_count - len(combined_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = len(combined_dataset)\n",
    "print(\"Before removing NaN values:\", row_count)\n",
    "combined_dataset = combined_dataset.dropna()\n",
    "print(\"Without duplicates: \", len(combined_dataset))\n",
    "print(\"Difference\", row_count - len(combined_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.rename(\n",
    "    columns={\n",
    "        \"Time (UTC)\": \"time_utc\",\n",
    "        \"Local hour of the day\": \"local_hour\",\n",
    "        \"Local weekend indicator\": \"is_weekend\",\n",
    "        \"Local month of the year\": \"local_month\",\n",
    "        \"Local year\": \"local_year\",\n",
    "        \"Temperature - Top 1 (K)\": \"year_temp_top1\",\n",
    "        \"Temperature - Top 3 (K)\": \"year_temp_top3\",\n",
    "        \"Monthly average temperature - Top 1 (K)\": \"monthly_temp_avg_top1\",\n",
    "        \"Monthly average temperature rank - Top 1\": \"monthly_temp_avg_rank_top1\",\n",
    "        \"Annual average temperature - Top 1 (K)\": \"year_temp_avg_top1\",\n",
    "        \"5 percentile temperature - Top 1 (K)\": \"year_temp_percentile_5\",\n",
    "        \"95 percentile temperature - Top 1 (K)\": \"year_temp_percentile_95\",\n",
    "        \"Annual electricity demand (TWh)\": \"year_electricity_demand\",\n",
    "        \"Annual electricity demand per capita (MWh)\": \"year_electricity_demand_per_capita_mwh\",\n",
    "        \"Load (MW)\": \"load_mw\",\n",
    "        \"GDP\": \"year_gdp_ppp\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset[\"year_electricity_demand_mw\"] = (\n",
    "    combined_dataset[\"year_electricity_demand\"] * 1000000\n",
    ")\n",
    "combined_dataset = combined_dataset.drop(columns=[\"year_electricity_demand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_amount_hours_region = []\n",
    "for name, group in combined_dataset.groupby([\"region_code\", \"local_year\"]):\n",
    "    list_amount_hours_region.append([name[0], name[1], len(group)])\n",
    "\n",
    "df_amount_hours_region = pandas.DataFrame(\n",
    "    list_amount_hours_region,\n",
    "    columns=[\"region_code\", \"local_year\", \"count_available_hours\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amount_hours_region[\"count_available_hours\"].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.to_parquet(\n",
    "    \"./data/combined_dataset.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pandas.read_parquet(\n",
    "    \"./data/combined_dataset.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Split into train, test, and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pandas.DataFrame()\n",
    "test_set_indices = []\n",
    "validation_set = pandas.DataFrame()\n",
    "validation_set_indices = []\n",
    "\n",
    "for name, group in combined_dataset.groupby(\"region_code\"):\n",
    "    max_year = group[\"local_year\"].max()\n",
    "\n",
    "    group_test_set = group[group[\"local_year\"] == max_year].copy()\n",
    "\n",
    "    test_set_indices.append(group_test_set.index)\n",
    "\n",
    "    test_set = pandas.concat([test_set, group_test_set], ignore_index=True)\n",
    "\n",
    "    group_val_set = group[group[\"local_year\"] == max_year - 1].copy()\n",
    "\n",
    "    validation_set_indices.append(group_val_set.index)\n",
    "\n",
    "    validation_set = pandas.concat(\n",
    "        [validation_set, group_val_set], ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set) / len(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_set) / len(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_set_indices = [\n",
    "    index for list_indicies in test_set_indices for index in list_indicies\n",
    "]\n",
    "all_val_set_indices = [\n",
    "    index\n",
    "    for list_indicies in validation_set_indices\n",
    "    for index in list_indicies\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop test and validation sets from the combined dataset\n",
    "print(len(combined_dataset))\n",
    "train_set = combined_dataset.drop(index=all_test_set_indices).copy()\n",
    "train_set = train_set.drop(index=all_val_set_indices)\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset: pandas.DataFrame):\n",
    "    \"\"\"\n",
    "    Process the dataset into splits to be used in training the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : pandas.DataFrame\n",
    "        Features for the model.\n",
    "    target : pandas.Series\n",
    "        Column with the target variable.\n",
    "    groups : pandas.Series\n",
    "        Column containing the region codes\n",
    "    \"\"\"\n",
    "    features = dataset[\n",
    "        [\n",
    "            \"local_hour\",\n",
    "            \"is_weekend\",\n",
    "            \"local_month\",\n",
    "            \"year_temp_top1\",\n",
    "            \"year_temp_top3\",\n",
    "            \"monthly_temp_avg_top1\",\n",
    "            \"monthly_temp_avg_rank_top1\",\n",
    "            \"year_temp_avg_top1\",\n",
    "            \"year_temp_percentile_5\",\n",
    "            \"year_temp_percentile_95\",\n",
    "            \"year_electricity_demand_per_capita_mwh\",\n",
    "            \"year_gdp_ppp\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    categorical_features = [\n",
    "        \"local_hour\",\n",
    "        \"is_weekend\",\n",
    "        \"local_month\",\n",
    "        \"monthly_temp_avg_rank_top1\",\n",
    "    ]\n",
    "\n",
    "    for cat_feature in categorical_features:\n",
    "        features[cat_feature] = features[cat_feature].astype(\"category\")\n",
    "\n",
    "    target = dataset[\"load_mw\"].copy()\n",
    "    groups = dataset[\"region_code\"].copy()\n",
    "\n",
    "    return features, target, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_target, train_groups = prepare_data(train_set)\n",
    "val_features, val_target, val_groups = prepare_data(validation_set)\n",
    "\n",
    "test_features, test_target, test_groups = prepare_data(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Scale the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_scaler = StandardScaler()\n",
    "train_target_scaler.fit(train_target.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_scaled = train_target_scaler.transform(\n",
    "    train_target.to_numpy().reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler for use in future predictions\n",
    "joblib.dump(train_target_scaler, \"./data/target_scaler.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    enable_categorical=True,\n",
    "    eval_metric=mean_absolute_percentage_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    train_features, train_target, eval_set=[(val_features, val_target)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.save_model(\"./data/xgboost_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost regressor for the scaled target variable\n",
    "xgb_model_scaled = XGBRegressor(\n",
    "    random_state=42,\n",
    "    enable_categorical=True,\n",
    "    eval_metric=mean_absolute_percentage_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Note: the validation features are not scaled yet TODO\n",
    "xgb_model_scaled.fit(\n",
    "    train_features, train_target_scaled, eval_set=[(val_features, val_target)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_scaled.save_model(\"./data/xgboost_model_scaled.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "#### Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_mape(\n",
    "    test_set, test_predictions, test_target, message: str = \"\"\n",
    ") -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcuate the mean absolute percentage error for the test set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with the region codes, years,\n",
    "        and mean absolute percentage errors for the test set.\n",
    "    \"\"\"\n",
    "    list_test_mae_values = []\n",
    "    for name, group in test_set.groupby([\"local_year\", \"region_code\"]):\n",
    "        current_mae = mean_absolute_percentage_error(\n",
    "            test_predictions[group.index], test_target.iloc[group.index]\n",
    "        )\n",
    "\n",
    "        list_test_mae_values.append([name[1], name[0], current_mae])\n",
    "\n",
    "    df_validation_mae_values = pandas.DataFrame(\n",
    "        list_test_mae_values, columns=[\"region_code\", \"year\", \"mape\"]\n",
    "    )\n",
    "\n",
    "    df_validation_mae_values.to_parquet(\n",
    "        \"data/test_mae_values\" + message + \".parquet\", engine=\"pyarrow\"\n",
    "    )\n",
    "    df_validation_mae_values.to_csv(\"data/test_mae_values\" + message + \".csv\")\n",
    "\n",
    "    return df_validation_mae_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set and calculate mae\n",
    "test_predictions = xgb_model.predict(test_features)\n",
    "calculate_test_mape(test_set, test_predictions, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on scaled test set and calculate mae\n",
    "test_predictions_scaled = xgb_model_scaled.predict(test_features)\n",
    "test_predictions_scaled = train_target_scaler.inverse_transform(\n",
    "    test_predictions_scaled.reshape(-1, 1)\n",
    ")\n",
    "calculate_test_mape(test_set, test_predictions_scaled, test_target, \"_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = xgb_model.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_test_mape(\n",
    "    train_set.reset_index(), train_predictions, train_target, \"_train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "#### Cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_results = cross_validate(\n",
    "    xgb_model,\n",
    "    train_features,\n",
    "    train_target,\n",
    "    groups=train_groups,\n",
    "    cv=LeaveOneGroupOut(),\n",
    "    scoring=[\"neg_mean_absolute_percentage_error\"],\n",
    "    return_train_score=True,\n",
    "    return_indices=True,\n",
    "    return_estimator=True,\n",
    "    n_jobs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"indices_train\"] = cv_results[\"indices\"][\"train\"]\n",
    "cv_results[\"indices_test\"] = cv_results[\"indices\"][\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'cv_results' is your original dictionary and\n",
    "# 'indices' is the key you want to exclude\n",
    "cv_results_filtered = {k: v for k, v in cv_results.items() if k != \"indices\"}\n",
    "\n",
    "# Create DataFrame from the filtered dictionary\n",
    "df_cv_results = pandas.DataFrame(cv_results_filtered)\n",
    "\n",
    "df_cv_results[\"test_mape\"] = -df_cv_results[\n",
    "    \"test_neg_mean_absolute_percentage_error\"\n",
    "]\n",
    "df_cv_results[\"train_mape\"] = -df_cv_results[\n",
    "    \"train_neg_mean_absolute_percentage_error\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_group_id = []\n",
    "for test_indices in cv_results[\"indices\"][\"test\"]:\n",
    "    list_test_group_id.append(train_groups.iloc[test_indices[0]])\n",
    "\n",
    "df_cv_results[\"group_id\"] = list_test_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_output = df_cv_results[\n",
    "    [\"group_id\", \"train_mape\", \"test_mape\", \"fit_time\", \"score_time\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_output.to_parquet(\"./data/cv_results.parquet\", engine=\"pyarrow\")\n",
    "df_cv_output.to_csv(\"./data/cv_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### Synthetic data for all collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_dataset = pandas.read_parquet(\n",
    "    \"./data/combined_dataset.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_xgb_model = XGBRegressor()\n",
    "trained_xgb_model.load_model(\"./data/xgboost_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_columns = trained_xgb_model.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = entire_dataset[input_features_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_xgb_model.predict(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = entire_dataset.drop(columns=input_features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset[\"predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = synthetic_dataset.drop(\n",
    "    columns=[\n",
    "        \"local_year\",\n",
    "        \"load_mw\",\n",
    "        \"year_electricity_demand_mw\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset.to_parquet(\n",
    "    \"./data/synthetic_dataset.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
