{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"DemandCast      Global hourly electricity demand forecasting  <p>Supported by</p> <p> </p>"},{"location":"#about","title":"About","text":"<p>DemandCast is a Python-based project focused on collecting, processing, and forecasting hourly electricity demand data. The aim of this project is to support energy planning studies by using machine learning models to generate hourly time series of future electricity demand or for countries without available data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Retrieval of open hourly and sub-hourly electricity demand data from public sources (ETL).</li> <li>Retrieval of weather and socio-economic data (ETL).</li> <li>Forecasting using machine learning models (models).</li> <li>Modular design for adding new countries or data sources.</li> <li>Support for reproducible, containerized development.</li> </ul> <p>The project is in active development, we are always looking for suggestions and contributions!</p>"},{"location":"#repository-structure","title":"Repository structure","text":"<pre><code>demandcast/\n\u251c\u2500\u2500 docs/                   # Project documentation (MkDocs)\n\u251c\u2500\u2500 ETL/                    # Scripts for extracting, transforming, and loading data\n\u251c\u2500\u2500 models/                 # Machine learning models for demand forecasting\n\u251c\u2500\u2500 .devcontainer/          # Development container configuration\n\u251c\u2500\u2500 .github/                # Github specifics such as actions\n\u251c\u2500\u2500 .gitignore              # File lists that git ignores\n\u251c\u2500\u2500 .pre-commit-config.yaml # Pre-commit configuration\n\u251c\u2500\u2500 .python-version         # Python version\n\u251c\u2500\u2500 CONTRIBUTING.md         # Guide to contributing\n\u251c\u2500\u2500 Dockerfile              # Docker setup for containerized runs\n\u251c\u2500\u2500 mkdocs.yml              # Documentation configuration file\n\u251c\u2500\u2500 pyproject.toml          # Project metadata and dependencies\n\u251c\u2500\u2500 ruff.toml               # Ruff configuration\n\u2514\u2500\u2500 uv.lock                 # Lockfile for project's dependencies\n</code></pre>"},{"location":"#data-collection-progress","title":"Data collection progress","text":"<p>Find the code that we used to retrieve the data in their respective files inside the ETL folder.</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/open-energy-transition/demandcast.git\ncd demandcast\n</code></pre>"},{"location":"#2-set-up-your-environment","title":"2. Set up your environment","text":"<p>This project uses <code>uv</code> as a pacakge manager. It can be used within the provided Dockerfile or installed standalone (see installing uv)</p> <pre><code>uv sync\n</code></pre> <p>Alternatively, you may use <code>conda</code> to install the packages listed in <code>pyproject.toml</code>.</p>"},{"location":"#3-run-scripts","title":"3. Run scripts","text":"<p>Scripts can be run directly using:</p> <pre><code>uv run path/to/script.py\n</code></pre> <p>If you have chosen <code>conda</code> as your package manager, you can run scripts with:</p> <pre><code>python path/to/script.py\n</code></pre> <p>Jupyter notebooks (details) can be launched with:</p> <pre><code>uv run --with jupyter jupyter lab --allow-root\n</code></pre>"},{"location":"#development-workflow","title":"Development workflow","text":""},{"location":"#run-tests-and-check-test-coverage","title":"Run tests and check test coverage","text":"<pre><code>uv run pytest\nuv run pytest --cov --cov-report term-missing\n</code></pre>"},{"location":"#pre-commit-and-lint-code","title":"Pre-commit and lint code","text":"<pre><code>uvx ruff format\nuvx ruff check --fix\nuvx mypy\nuvx pre-commit\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is currently hosted on GitHub pages. It is built with mkdocs.</p> <p>To run it locally:</p> <pre><code>uv run mkdocs serve\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions in the form of:</p> <ul> <li>Country-specific ETL modules</li> <li>New or improved forecasting models</li> <li>Documentation and testing enhancements</li> </ul> <p>Please follow the repository\u2019s structure and submit your changes via pull request.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0).</p>"},{"location":"Deployment/","title":"Deployment","text":"<p>This section is a work in progress.</p> <p>It will feature advice on how to deploy the containers to the cloud, we are working on releasing those first.</p>"},{"location":"ETL/ETL/","title":"Extract Transform Load (ETL)","text":"<p>This module contains all scripts related to the extraction, transformation, and loading of electricity demand, weather, and population data. It is designed to provide a standardized pipeline to prepare data for downstream modeling and analysis.</p>"},{"location":"ETL/ETL/#overview","title":"Overview","text":"<p>The ETL process consists of four main stages:</p> 1. Fetch the data <p>Retrieve raw data from online sources or APIs.</p> 2. Transform into tabular format <p>Convert raw data into structured, tabular (Parquet-compatible) formats.</p> 3. Data cleaning <p>Ensure time synchronization and unit consistency.</p> 4. Save processed data <p>Export cleaned data to local or cloud storage in Parquet or CSV format.</p>"},{"location":"ETL/ETL/#structure","title":"Structure","text":"<pre><code>ETL/\n\u251c\u2500\u2500 download_electricity_data.py    # Main script to download electricity demand data\n\u251c\u2500\u2500 download_population_data.py     # Script to retrieve population data from SEDAC\n\u251c\u2500\u2500 download_weather_data.py        # Script to retrieve weather data from Copernicus\n\u251c\u2500\u2500 get_temperature_data.py         # Script to extract temperature time series\n\u251c\u2500\u2500 retrieval/                      # Data source-specific scripts and configuration\n\u2502   \u251c\u2500\u2500 entsoe.py, eia.py, ...      # Retrieval logic for each data provider\n\u2502   \u251c\u2500\u2500 entsoe.yaml, eia.yaml, ...  # Lists of country/subdivision information per source\n\u2502   \u2514\u2500\u2500 copernicus.py               # Copernicus Climate Data Store (CDS) retrieval functions\n\u251c\u2500\u2500 shapes/                         # Non-standard subdivision shapes\n\u2502   \u2514\u2500\u2500 eia.py, ons.py, ...         # Scripts that generate non-standard shapefiles\n\u251c\u2500\u2500 util/                           # Shared utilities\n\u2502   \u251c\u2500\u2500 directories.py              # Functions to read directories\n\u2502   \u251c\u2500\u2500 directories.yaml            # Keys to define the ETL folder structure\n\u2502   \u251c\u2500\u2500 entities.py                 # Functions to read country and subdivision information\n\u2502   \u251c\u2500\u2500 fetcher.py                  # Functions to fetch online content\n\u2502   \u251c\u2500\u2500 figures.py                  # Functions to plot basic figures\n\u2502   \u251c\u2500\u2500 geospatial.py               # Functions to process geospatial data\n\u2502   \u251c\u2500\u2500 shapes.py                   # Functions to read country and subdivision shapes\n\u2502   \u251c\u2500\u2500 time_series.py              # Time series processing\n\u2502   \u2514\u2500\u2500 uploader.py                 # Functions for cloud storage uploads\n\u2514\u2500\u2500 .env                            # API keys (not included in repo)\n</code></pre>"},{"location":"ETL/ETL/#application-programming-interface-api-keys","title":"Application Programming Interface (API) keys","text":"<p>Some scripts require API keys to access data from external services. These keys should be stored in a <code>.env</code> file in the <code>ETL/</code> directory. The <code>.env</code> file should not be included in the repository and should contain the following environment variables:</p> <pre><code>CDS_API_KEY=&lt;your_key&gt;             # For data retrieval from Copernicus CDS\nENTSOE_API_KEY=&lt;your_key&gt;          # For data retrieval from ENTSO-E\nEIA_API_KEY=&lt;your_key&gt;             # For data retrieval from EIA\nZENODO_API_KEY=&lt;your_key&gt;          # For data upload to Zenodo\nSANDBOX_ZENODO_API_KEY=&lt;your_key&gt;  # For data upload to Zenodo Sandbox\n</code></pre>"},{"location":"ETL/ETL/#electricity-demand-data","title":"Electricity demand data","text":"<p>Scripts in this section download and process electricity demand data from multiple sources such as ENTSO-E, EIA, and CCEI. The data is processed to have all timestamps in UTC and electricity demand in MW.</p>"},{"location":"ETL/ETL/#main-script","title":"Main script","text":"<p>Run the main script with:</p> <pre><code>uv run download_electricity_data.py &lt;data_source&gt; [-c country_or_subdivision_code] [-f code_file] [-g bucket_name] [-z] [-p]\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;data_source&gt;</code>: The acronym of the data source as defined in the retrieval modules (e.g., <code>ENTSOE</code>).</li> <li><code>-c, --code</code>: (Optional) The ISO Alpha-2 code (e.g., <code>FR</code>) or a combination of ISO Alpha-2 code and subdivision code (e.g., <code>US_CAL</code>).</li> <li><code>-f, --file</code>: (Optional) The path to the YAML file containing the list of codes for the countries and subdivisions of interest.</li> <li><code>-g, --upload_to_gcs</code>: (Optional) The bucket name of the Google Cloud Storage (GCS) to upload the data.</li> <li><code>-z, --upload_to_zenodo</code>: (Optional) If set, the script will upload the data to a new or existing Zenodo record.</li> <li><code>-p, --publish_to_zenodo</code>: (Optional) If set, the script will publish the Zenodo record after uploading.</li> </ul>"},{"location":"ETL/ETL/#example","title":"Example","text":"<p>Download electricity data for France from ENTSO-E:</p> <pre><code>uv run download_electricity_data.py ENTSOE -c FR\n</code></pre>"},{"location":"ETL/ETL/#retrieval-scripts","title":"Retrieval scripts","text":"<p>Each retrieval script in the <code>retrieval/</code> folder is designed to fetch electricity demand data from a specific data source. The main functions in each script typically include:</p> <ul> <li>Check input parameters (<code>_check_input_parameters</code>): Checks that the input parameters are valid.</li> <li>Data request construction (<code>get_available_requests</code>): Builds all data requests based on the availability of the data source.</li> <li>URL construction (<code>get_url</code>): Generates the appropriate web request URL.</li> <li>Data download and processing (<code>download_and_extract_data_for_request</code>): Fetches the data using <code>utils.fetcher</code> functions and transforms it into a <code>pandas.Series</code>.</li> </ul>"},{"location":"ETL/ETL/#names-codes-time-zones-and-data-time-ranges-for-countries-and-subdivisions","title":"Names, codes, time zones, and data time ranges for countries and subdivisions","text":"<p>For each retrieval script in the <code>retrieval/</code> folder, a corresponding YAML file must be created. The YAML file should contain a list of dictionaries, each representing a country or subdivision from the respective data source. The following rules apply:</p> <ul> <li>Names and codes should adhere to the ISO 3166 standard.</li> <li>For countries and standard subdivisions, use alpha-2 codes.</li> <li>For non-standard subdivisions, use a widely accepted name and code.</li> <li>Data time range must be specified.</li> <li>For subdivisions, time zone must be specified.</li> </ul>"},{"location":"ETL/ETL/#non-standard-subdivisions","title":"Non-standard subdivisions","text":"<p>Some countries have subdivisions that are not standard ISO subdivisions. For these cases, the <code>shapes/</code> folder contains scripts to generate the shapes of these subdivisions. The scripts are named after the data source (e.g., <code>eia.py</code>, <code>ons.py</code>) and contain functions to generate the shapes. The generated shapes are then used in the retrieval scripts and for plotting.</p>"},{"location":"ETL/ETL/#population-data","title":"Population data","text":"<p>To download and prepare population data from the Socioeconomic Data and Applications Center (SEDAC):</p> <pre><code>uv run download_population_data.py [-c country_or_subdivision_code] [-f code_file] [-y year]\n</code></pre> <p>Arguments:</p> <ul> <li><code>-c, --code</code>: (Optional) The ISO Alpha-2 code (e.g., <code>FR</code>) or a combination of ISO Alpha-2 code and subdivision code (e.g., <code>US_CAL</code>).</li> <li><code>-f, --file</code>: (Optional) The path to the YAML file containing the list of codes for the countries and subdivisions of interest.</li> <li><code>-y, --year</code>: (Optional) The year of the population data to be downloaded.</li> </ul> <p>The script:</p> <ul> <li>Downloads 30-second resolution data from SEDAC.</li> <li>Aggregates to 0.25\u00b0 resolution to match weather data.</li> <li>Saves <code>.nc</code> files in <code>data/population_density/</code>.</li> </ul>"},{"location":"ETL/ETL/#weather-data","title":"Weather data","text":"<p>To retrieve weather data from the Copernicus Climate Data Store (CDS), first ensure that you are registered on the website and have your API key stored in the <code>.env</code> file. Instructions for the API key can be found here. Then run:</p> <pre><code>uv run download_weather_data.py [-c country_or_subdivision_code] [-f code_file] [-y year]\n</code></pre> <p>Arguments:</p> <ul> <li><code>-c, --code</code>: (Optional) The ISO Alpha-2 code (e.g., <code>FR</code>) or a combination of ISO Alpha-2 code and subdivision code (e.g., <code>US_CAL</code>).</li> <li><code>-f, --file</code>: (Optional) The path to the YAML file containing the list of codes for the countries and subdivisions of interest.</li> <li><code>-y, --year</code>: (Optional) The year of the weather data to be downloaded.</li> </ul> <p>The script:</p> <ul> <li>Retrieves temperature data from the Copernicus Climate Data Store.</li> <li>Stores <code>.nc</code> files in <code>data/weather/</code>.</li> </ul> <p>Note that the size of weather data files is on the order of 100 MB per country per year, so ensure you have sufficient storage space.</p>"},{"location":"ETL/ETL/#gross-domestic-product-gdp-data","title":"Gross Domestic Product (GDP) data","text":"<p>To retrieve gridded GDP data, run:</p> <pre><code>uv run download_gdp_data.py [-c country_or_subdivision_code] [-f code_file] [-y year]\n</code></pre> <p>Arguments:</p> <ul> <li><code>-c, --code</code>: (Optional) The ISO Alpha-2 code (e.g., <code>FR</code>) or a combination of ISO Alpha-2 code and subdivision code (e.g., <code>US_CAL</code>).</li> <li><code>-f, --file</code>: (Optional) The path to the YAML file containing the list of codes for the countries and subdivisions of interest.</li> <li><code>-y, --year</code>: (Optional) The year of the GDP data to be downloaded.</li> </ul> <p>The script will download GDP data from Zenodo and store it in <code>data/gdp/</code>.</p>"},{"location":"ETL/ETL/#temperature-time-series-extraction","title":"Temperature time series extraction","text":"<p>Generate temperature time series based on population-weighted regions:</p> <pre><code>uv run ETL/get_temperature_data.py\n</code></pre> <p>Arguments:</p> <ul> <li><code>-c, --code</code>: (Optional) The ISO Alpha-2 code (e.g., <code>FR</code>) or a combination of ISO Alpha-2 code and subdivision code (e.g., <code>US_CAL</code>).</li> <li><code>-f, --file</code>: (Optional) The path to the YAML file containing the list of codes for the countries and subdivisions of interest.</li> <li><code>-y, --year</code>: (Optional) The year of the weather data to use.</li> </ul> <p>The script will extract time series of temperature based on the largest and three largest population density areas and output <code>.csv</code> or <code>.parquet</code> files in <code>data/temperature/</code>.</p> <p>Note that the <code>get_temperature_data.py</code> script requires both weather and population data to be available in the specified directories.</p>"},{"location":"Models/","title":"Models","text":"<p>This is the documentation of the models used in this repository.</p>"},{"location":"Models/#top-down-approach","title":"Top-down approach","text":"<p>XGBoost</p>"},{"location":"Models/xgboost/","title":"XGBoost","text":"<p>\"XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework.\" (XGBoost documentation)</p>"},{"location":"Models/xgboost/#motivation","title":"Motivation","text":"<p>The core motivation for using XGBoost to generate hourly electricity demand forecasts is due to previous work in literature. Our approach involves using socioeconomic and weather parameters passed to an XGBoost model to predict the hourly electricity demand. For this purpose it is a fast model to train and perform inference, therefore serves as a great option for a baseline that can be expanded on in future work.</p>"},{"location":"Models/xgboost/#features","title":"Features","text":"<p>The following features are used by the model. See <code>serve.py</code> for more details:</p> <pre><code>class PredictionInput(BaseModel):\n    \"\"\"Define input data for prediction.\"\"\"\n\n    is_weekend: int = Field(ge=0, le=1)\n    hour: int = Field(ge=0, lt=24)\n    month: int = Field(ge=1, le=12)\n    month_temp_avg: float\n    month_temp_rank: int = Field(ge=1, le=12)\n    year_electricity_demand_per_capita: float\n    year_gdp_ppp: float = Field(gt=0)\n    year_temp_percentile_5: float\n    year_temp_percentile_95: float\n    year_temp_top3: float\n</code></pre> Temporal Hour of the day <p><code>hour: int = Field(ge=0, lt=24)</code></p> Month of the year <p><code>month: int = Field(ge=1, le=12)</code></p> Weekend indicator <p><code>is_weekend: int = Field(ge=0, le=1)</code></p> Electricity Demand Yearly electricity demand per capita <p><code>year_electricity_demand_per_capita: float</code></p> Monetary Yearly Gross Domestic Product Purchasing Power Parity <p><code>year_gdp_ppp: float = Field(gt=0)</code></p> Weather <p>The grid cells used for these features has a resolution of (0.25\u00b0 x 0.25\u00b0) and is bounded by the respective country borders.</p> Average temperature for the month <p><code>month_temp_avg: float</code></p> <p>Calculated based on the temperature in the most populous grid cell.</p> Temperature rank of the month <p><code>month_temp_rank: int = Field(ge=1, le=12)</code></p> <p>Calculated based on the temperature in the most populous grid cell.</p> Yearly temperature percentiles <p><code>year_temp_percentile_5: float</code></p> <p><code>year_temp_percentile_95: float</code></p> <p>Calculated based on the temperature in the most populous grid cell.</p> Average temperature in most populous grid cell <p><code>year_temp_top1: float</code></p> Average temperature in most populous 3 grid cells <p><code>year_temp_top3: float</code></p>"},{"location":"Models/xgboost/#implementation","title":"Implementation","text":"<p>You can find all the relevant files in the <code>models/xgboost</code> folder.</p>"},{"location":"Models/xgboost/#inferencepy","title":"inference.py","text":"<p>Run inference for an XGBoost model that outputs electricity demand.</p> <p>This module loads the input data, loads the pre-trained XGBoost model, performs inference using the loaded model, and saves the results.</p>"},{"location":"Models/xgboost/#servepy","title":"serve.py","text":"<p>Serve an XGBoost model for electricity demand prediction using FastAPI.</p> <p>This module provides a REST API service to serve predictions from a pre-trained XGBoost model.</p> <p>It includes endpoints for health checks, model information, and making predictions based on passed features.</p>"},{"location":"Models/xgboost/#xgboostipynb","title":"XGBoost.ipynb","text":"<p>A jupyter notebook containing the code to train and run an XGBoost model based on toktarova(2019) data. Includes visualizations and cross-validation.</p>"}]}